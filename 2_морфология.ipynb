{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "САД NLP 2 | морфология.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oserikov/data-science-nlp/blob/master/2_%D0%BC%D0%BE%D1%80%D1%84%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIkpnJD7X3Xj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip -qq install yargy --progress-bar off\n",
        "!pip -qq install pymorphy2 --progress-bar off\n",
        "!pip -qq install -U PyYAML --progress-bar off\n",
        "!pip -qq install rnnmorph --progress-bar off\n",
        "!pip -qq install rusenttokenize --progress-bar off\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from collections import defaultdict as dd\n",
        "from operator import itemgetter\n",
        "from pymorphy2.tokenizers import simple_word_tokenize as word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from rusenttokenize import ru_sent_tokenize\n",
        "import string\n",
        "import pymorphy2\n",
        "from rnnmorph.predictor import RNNMorphPredictor\n",
        "\n",
        "\n",
        "import warnings\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "warnings.warn = warn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JQXhd1x0f7LE"
      },
      "source": [
        "## Морфологический анализ\n",
        "\n",
        "Задачи морфологического анализа:\n",
        "\n",
        "* Разбор слова — определение нормальной формы (леммы), основы (стема) и грамматических характеристик слова\n",
        "* Синтез словоформы — генерация словоформы по заданным грамматическим характеристикам из леммы\n",
        "\n",
        "Морфологический анализ — не самая сильная сторона NLTK.\n",
        "\n",
        "## POS-tagging\n",
        "\n",
        "**Частеречная разметка**, или **POS-tagging** _(part of speech tagging)_ —  определение части речи и грамматических характеристик слов в тексте (корпусе) с приписыванием им соответствующих тегов.\n",
        "\n",
        "Для большинства слов возможно несколько разборов (т.е. несколько разных лемм, несколько разных частей речи и т.п.). Теггер генерирует  все варианты, ранжирует их по вероятности и по умолчанию выдает наиболее вероятный. Выбор одного разбора из нескольких называется **снятием омонимии**, или **дизамбигуацией**.\n",
        "\n",
        "### Наборы тегов\n",
        "\n",
        "Существует множество наборов грамматических тегов, или тегсетов:\n",
        "* НКРЯ\n",
        "* Mystem\n",
        "* UPenn\n",
        "* OpenCorpora (его использует pymorphy2)\n",
        "* Universal Dependencies\n",
        "* ...\n",
        "\n",
        "Есть даже [библиотека](https://github.com/kmike/russian-tagsets) для преобразования тегов из одной системы в другую для русского языка, `russian-tagsets`. Но важно помнить, что преобразования бывают с потерями.\n",
        "\n",
        "На данный момент стандартом является **Universal Dependencies**. Подробнее про проект можно почитать [вот тут](http://universaldependencies.org/), а про теги — [вот тут](http://universaldependencies.org/u/pos/). Вот список основных (частереных) тегов UD:\n",
        "\n",
        "* ADJ: adjective\n",
        "* ADP: adposition\n",
        "* ADV: adverb\n",
        "* AUX: auxiliary\n",
        "* CCONJ: coordinating conjunction\n",
        "* DET: determiner\n",
        "* INTJ: interjection\n",
        "* NOUN: noun\n",
        "* NUM: numeral\n",
        "* PART: particle\n",
        "* PRON: pronoun\n",
        "* PROPN: proper noun\n",
        "* PUNCT: punctuation\n",
        "* SCONJ: subordinating conjunction\n",
        "* SYM: symbol\n",
        "* VERB: verb\n",
        "* X: other\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QICje2dEgLCw",
        "colab_type": "text"
      },
      "source": [
        "### pymystem3\n",
        "\n",
        "**pymystem3** — это питоновская обертка для яндексовского морфологичекого анализатора Mystem. Его можно скачать отдельно и использовать из консоли. Может работать с незнакомыми словами (out-of-vocabulary words, OOV).\n",
        "\n",
        "**pymystem3 не работает в google colaboratory.**\n",
        "\n",
        "* [Документация Mystem](https://tech.yandex.ru/mystem/doc/index-docpage/)\n",
        "* [Документация pymystem3](http://pythonhosted.org/pymystem3/)\n",
        "\n",
        "Инициализируем Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
        "* mystem_bin - путь к `mystem`, если их несколько\n",
        "* grammar_info - нужна ли грамматическая информация или только леммы (по дефолту нужна)\n",
        "* disambiguation - нужно ли снятие омонимии - дизамбигуация (по дефолту нужна)\n",
        "* entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по дефолту оставляется все)\n",
        "\n",
        "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0FElWVK47AT",
        "colab_type": "text"
      },
      "source": [
        "###  pymorphy2\n",
        "\n",
        "**pymorphy2** — это полноценный морфологический анализатор, целиком написанный на Python. Он также умеет ставить слова в нужную форму (спрягать и склонять). Может работать с незнакомыми словами (out-of-vocabulary words, OOV).\n",
        "\n",
        "[Документация pymorphy2](https://pymorphy2.readthedocs.io/en/latest/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "913cWQ-8hiCA",
        "colab_type": "text"
      },
      "source": [
        "### rnnmorph\n",
        "**rnnmorph** &mdash; это морфологический анализатор на нейросетях, занявший первое место на дорожке по морофологическому анализу [\"Диалога 2017\"](http://www.dialog-21.ru/evaluation/2017/morphology/). \n",
        "\n",
        "Предлагает меньшее количество тегов в разборе.\n",
        "\n",
        "Работает заметно медленнее, чем pymorphy и mystem.\n",
        "\n",
        "[Документация rnnmorph](https://github.com/IlyaGusev/rnnmorph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk1-wejHiXjq",
        "colab_type": "text"
      },
      "source": [
        "### maru\n",
        "**maru** — это морфологический анализатор на нейросетях, в документации указаны результаты чуть лучше, чем у rnnmorph на той же задаче.\n",
        "\n",
        "Тоже медленный\n",
        "\n",
        "[Документация maru](https://github.com/chomechome/maru)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH_O8y9-i9xR",
        "colab_type": "text"
      },
      "source": [
        "### Классификация новостей Ленты по темам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p64AMTh8X5lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget -q -O lenta-ru-news-part.csv https://www.dropbox.com/s/ja23c9l1ppo9ix7/lenta-ru-news-part.csv?dl=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdKArr-RZ8Pj",
        "colab_type": "code",
        "outputId": "0e410455-8587-4f45-ad99-d7e00159f372",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "lenta = pd.read_csv('lenta-ru-news-part.csv', usecols=['title', 'text', 'topic'])\n",
        "lenta[\"topic_cat\"] = lenta.topic.astype('category').cat.codes\n",
        "\n",
        "def reload_lenta_dataset():\n",
        "    global lenta\n",
        "    lenta = pd.read_csv('lenta-ru-news-part.csv', usecols=['title', 'text', 'topic'])\n",
        "    lenta = lenta.sample(frac=1).reset_index(drop=True).dropna(subset = ['text', 'topic'])\n",
        "    lenta[\"topic_cat\"] = lenta.topic.astype('category').cat.codes\n",
        "\n",
        "\n",
        "\n",
        "lenta.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>topic_cat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Австрия не представила доказательств вины росс...</td>\n",
              "      <td>Австрийские правоохранительные органы не предс...</td>\n",
              "      <td>Спорт</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Овечкин повторил свой рекорд</td>\n",
              "      <td>Капитан «Вашингтона» Александр Овечкин сделал...</td>\n",
              "      <td>Спорт</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Названы регионы России с самым дорогим и дешев...</td>\n",
              "      <td>Производитель онлайн-касс «Эвотор» проанализир...</td>\n",
              "      <td>Экономика</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Россию и Украину пригласили на переговоры по газу</td>\n",
              "      <td>Вице-президент Еврокомиссии Марош Шефчович при...</td>\n",
              "      <td>Экономика</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Хоккеист НХЛ забросил шайбу с отрицательного угла</td>\n",
              "      <td>Нападающий клуба «Эдмонтон Ойлерс» Коннор Макд...</td>\n",
              "      <td>Спорт</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... topic_cat\n",
              "0  Австрия не представила доказательств вины росс...  ...         3\n",
              "1                       Овечкин повторил свой рекорд  ...         3\n",
              "2  Названы регионы России с самым дорогим и дешев...  ...         4\n",
              "3  Россию и Украину пригласили на переговоры по газу  ...         4\n",
              "4  Хоккеист НХЛ забросил шайбу с отрицательного угла  ...         3\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu4Dbciok_9_",
        "colab_type": "code",
        "outputId": "e203ce64-3547-4aec-cbd7-f77676557b11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "lenta.topic.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Экономика          79538\n",
              "Спорт              64421\n",
              "Культура           53803\n",
              "Наука и техника    53136\n",
              "Бизнес              7399\n",
              "Name: topic, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClR-YWl4lC5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_test_split(source_col, target_col, source_features_encoder):\n",
        "    X = source_features_encoder(source_col)\n",
        "    y = target_col\n",
        "    \n",
        "    training_size = len(source_col)*70//100\n",
        "    X_train, y_train = X[:training_size], y[:training_size]\n",
        "    X_test, y_test = X[training_size:], y[training_size:]\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def run_experiment(dataset, dataset_part_size, source_features_encoder):\n",
        "    \n",
        "    \n",
        "    print(len(dataset))\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for i in range(5):\n",
        "        dataset_shuf = dataset.sample(frac=1)[:dataset_part_size]\n",
        "        print(len(dataset_shuf))\n",
        "\n",
        "        X_train, y_train, X_test, y_test = train_test_split(dataset_shuf.text, dataset_shuf.topic_cat, source_features_encoder)\n",
        "\n",
        "        clf = LogisticRegression()\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        predictions = clf.predict(X_test)\n",
        "\n",
        "        precision = precision_score(y_test.values, predictions, average='weighted')\n",
        "        recall = recall_score(y_test.values, predictions, average='weighted')\n",
        "        f1 = f1_score(y_test.values, predictions, average='weighted')\n",
        "\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        print(f\"precision {str(precision)}\")\n",
        "        print(f\"recall {str(recall)}\")\n",
        "        print(f\"f1-score {str(f1)}\")\n",
        "\n",
        "    print('*' * 10)\n",
        "    print(\"mean precision\", np.mean(precision_scores))\n",
        "    print(\"mean recall\", np.mean(recall_scores))\n",
        "    print(\"mean f1-score\", np.mean(f1_scores))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf5BH0rAlbNY",
        "colab_type": "text"
      },
      "source": [
        "### наивное решение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5ZFnwHKkBM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "\n",
        "def naive_features_tfidf_encoder(text_col):\n",
        "    return tfidf_vectorizer.fit_transform(text_col)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U4yLtJgmYkJ",
        "colab_type": "code",
        "outputId": "2b8478cd-c9e2-47eb-a557-559f717cad09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "run_experiment(lenta, 5000, naive_features_tfidf_encoder)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "258297\n",
            "5000\n",
            "precision 0.8992486561111459\n",
            "recall 0.9266666666666666\n",
            "f1-score 0.9115150924166612\n",
            "5000\n",
            "precision 0.9198654085071345\n",
            "recall 0.9393333333333334\n",
            "f1-score 0.9282856317362543\n",
            "5000\n",
            "precision 0.9126459128746067\n",
            "recall 0.9366666666666666\n",
            "f1-score 0.9234252689200926\n",
            "5000\n",
            "precision 0.8990482969104359\n",
            "recall 0.9266666666666666\n",
            "f1-score 0.911275289156526\n",
            "5000\n",
            "precision 0.9133780100337434\n",
            "recall 0.9366666666666666\n",
            "f1-score 0.9240829650569145\n",
            "**********\n",
            "mean precision 0.9088372568874131\n",
            "mean recall 0.9332\n",
            "mean f1-score 0.9197168494572898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjd9oYKSmYnf",
        "colab_type": "text"
      },
      "source": [
        "**Ого!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFKfxThkmV4L",
        "colab_type": "text"
      },
      "source": [
        "### данные посложнее"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cdtm-x74zUA8",
        "colab": {}
      },
      "source": [
        "!wget -q https://github.com/BobaZooba/HSE-Deep-Learning-in-NLP-Course/blob/master/week_05/data/answers_subsample.csv?raw=true -O data.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRJMuutO3LXZ",
        "colab_type": "code",
        "outputId": "ca2862c4-35bd-4e1f-ab50-0af6f9b42122",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "data = pd.read_csv('data.csv')\n",
        "data.columns=['topic', 'text']\n",
        "data[\"topic_cat\"] = data.topic.astype('category').cat.codes\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>text</th>\n",
              "      <th>topic_cat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>business</td>\n",
              "      <td>Могут ли в россельхозбанке дать в залог норков...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>law</td>\n",
              "      <td>Может ли срочник перевестись на контракт после...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>business</td>\n",
              "      <td>Продажа недвижимости по ипотеки ? ( арестованы...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>business</td>\n",
              "      <td>В чем смысл криптовалюты, какая от неё выгода ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>law</td>\n",
              "      <td>часть 1 статья 158 похитил телефон</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      topic                                               text  topic_cat\n",
              "0  business  Могут ли в россельхозбанке дать в залог норков...          0\n",
              "1       law  Может ли срочник перевестись на контракт после...          2\n",
              "2  business  Продажа недвижимости по ипотеки ? ( арестованы...          0\n",
              "3  business  В чем смысл криптовалюты, какая от неё выгода ...          0\n",
              "4       law                 часть 1 статья 158 похитил телефон          2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlY4qqwDy-Q9",
        "colab_type": "code",
        "outputId": "bc073795-d26c-452b-a863-e4a0afda3b37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "run_experiment(data, 50000, naive_features_tfidf_encoder)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "237779\n",
            "50000\n",
            "precision 0.7653119634205529\n",
            "recall 0.7617333333333334\n",
            "f1-score 0.759140827826239\n",
            "50000\n",
            "precision 0.755805469854021\n",
            "recall 0.7494666666666666\n",
            "f1-score 0.7473729674473154\n",
            "50000\n",
            "precision 0.7665933673550389\n",
            "recall 0.7638\n",
            "f1-score 0.7614770959056998\n",
            "50000\n",
            "precision 0.7621465103987121\n",
            "recall 0.758\n",
            "f1-score 0.7555892118494607\n",
            "50000\n",
            "precision 0.7612214526115553\n",
            "recall 0.757\n",
            "f1-score 0.7546073388418562\n",
            "**********\n",
            "mean precision 0.762215752727976\n",
            "mean recall 0.7580000000000001\n",
            "mean f1-score 0.7556374883741143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2neLlkFVzm--",
        "colab_type": "text"
      },
      "source": [
        "### может, нам поможет морфология?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHXUIpYG4Bom",
        "colab_type": "code",
        "outputId": "38682c10-3f0d-40fc-b367-e646ab55ff38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "morph_analyzer = pymorphy2.MorphAnalyzer()\n",
        "russian_stopwords = stopwords.words('russian')\n",
        "\n",
        "tokenization_results = {}\n",
        "\n",
        "def pymorphy_preprocess_tokenize(text):\n",
        "    \n",
        "    text_preprocessed_tokenized = []\n",
        "        \n",
        "    for sentence in ru_sent_tokenize(text):\n",
        "        \n",
        "        clean_words = [word.strip(string.punctuation) for word in word_tokenize(text)]\n",
        "        clean_words = [word for word in clean_words if word]\n",
        "        clean_words = [word.lower() for word in clean_words if word]\n",
        "        clean_words = [word for word in clean_words if word not in russian_stopwords]\n",
        "        \n",
        "        clean_lemmas = []\n",
        "        for word in clean_words:\n",
        "            lemma = tokenization_results.get(word, None) \n",
        "            if lemma is None:\n",
        "                lemma = morph_analyzer.parse(word)[0].normal_form\n",
        "            tokenization_results[word] = lemma\n",
        "            clean_lemmas.append(lemma)\n",
        "        \n",
        "        text_preprocessed_tokenized.extend(clean_lemmas)\n",
        "\n",
        "    return text_preprocessed_tokenized\n",
        "\n",
        "pymorphy_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), tokenizer=pymorphy_preprocess_tokenize)\n",
        "def pymorphy_features_tfidf_encoder(text_col):\n",
        "    return pymorphy_tfidf_vectorizer.fit_transform(text_col)\n",
        "\n",
        "run_experiment(data, 50000, pymorphy_features_tfidf_encoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "237779\n",
            "50000\n",
            "precision 0.7950074806864773\n",
            "recall 0.7916666666666666\n",
            "f1-score 0.7906428054929162\n",
            "50000\n",
            "precision 0.7945388355749257\n",
            "recall 0.7915333333333333\n",
            "f1-score 0.7904031318755138\n",
            "50000\n",
            "precision 0.801018428560814\n",
            "recall 0.7968666666666666\n",
            "f1-score 0.7963669009195034\n",
            "50000\n",
            "precision 0.7967151285353029\n",
            "recall 0.7928\n",
            "f1-score 0.7915556436382343\n",
            "50000\n",
            "precision 0.8019004270567335\n",
            "recall 0.7982\n",
            "f1-score 0.7977549701820409\n",
            "**********\n",
            "mean precision 0.7978360600828507\n",
            "mean recall 0.7942133333333333\n",
            "mean f1-score 0.7933446904216417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3viV_Uwl4-NM",
        "colab_type": "code",
        "outputId": "4208cbc9-456d-4289-e3ec-92235e3d9bb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "run_experiment(data, 1000, naive_features_tfidf_encoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "237779\n",
            "1000\n",
            "precision 0.5344938124017621\n",
            "recall 0.42\n",
            "f1-score 0.3356690838262896\n",
            "1000\n",
            "precision 0.46958754813895004\n",
            "recall 0.42333333333333334\n",
            "f1-score 0.36455190448325464\n",
            "1000\n",
            "precision 0.5504995393403352\n",
            "recall 0.47333333333333333\n",
            "f1-score 0.4266368759642257\n",
            "1000\n",
            "precision 0.5870619777097482\n",
            "recall 0.42\n",
            "f1-score 0.37016441896441904\n",
            "1000\n",
            "precision 0.6281292041292041\n",
            "recall 0.44\n",
            "f1-score 0.3817159307751757\n",
            "**********\n",
            "mean precision 0.553954416344\n",
            "mean recall 0.43533333333333335\n",
            "mean f1-score 0.375747642802673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWBAsiqlGtU-",
        "colab_type": "code",
        "outputId": "748afbe8-13af-4d2a-8c83-d039de3d63ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "run_experiment(data, 1000, pymorphy_features_tfidf_encoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "237779\n",
            "1000\n",
            "precision 0.7249940801457195\n",
            "recall 0.4866666666666667\n",
            "f1-score 0.43055405774160116\n",
            "1000\n",
            "precision 0.5905386952229058\n",
            "recall 0.49\n",
            "f1-score 0.46024582602948366\n",
            "1000\n",
            "precision 0.7150833333333334\n",
            "recall 0.4533333333333333\n",
            "f1-score 0.42506535947712415\n",
            "1000\n",
            "precision 0.562933736585256\n",
            "recall 0.4633333333333333\n",
            "f1-score 0.41780203212311584\n",
            "1000\n",
            "precision 0.6767779453661807\n",
            "recall 0.4766666666666667\n",
            "f1-score 0.4235553277174007\n",
            "**********\n",
            "mean precision 0.6540655581306791\n",
            "mean recall 0.47400000000000003\n",
            "mean f1-score 0.43144452061774513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42b8yNA-5kQE",
        "colab_type": "text"
      },
      "source": [
        "### rnnmorph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5dE8PoQIfCO",
        "colab_type": "code",
        "outputId": "842cffef-5d8a-4290-9ad8-925c15dad995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "\n",
        "predictor = RNNMorphPredictor(language=\"ru\")\n",
        "\n",
        "\n",
        "\n",
        "def rnnmorph_preprocess_tokenize(text):\n",
        "    \n",
        "    text_preprocessed_tokenized = []\n",
        "        \n",
        "    for sentence in ru_sent_tokenize(text):\n",
        "        \n",
        "        clean_words = [word.strip(string.punctuation) for word in word_tokenize(text)]\n",
        "        clean_words = [word for word in clean_words if word]\n",
        "        clean_words = [word.lower() for word in clean_words if word]\n",
        "        \n",
        "        clean_lemmas = [analysis.pos + '_' + analysis.normal_form for analysis in predictor.predict(clean_words)]\n",
        "        text_preprocessed_tokenized.extend(clean_lemmas)\n",
        "\n",
        "    return text_preprocessed_tokenized\n",
        "\n",
        "\n",
        "\n",
        "rnnmorph_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), tokenizer=rnnmorph_preprocess_tokenize)\n",
        "def rnnmorph_features_tfidf_encoder(text_col):\n",
        "    return rnnmorph_tfidf_vectorizer.fit_transform(text_col)\n",
        "\n",
        "run_experiment(data, 1000, rnnmorph_features_tfidf_encoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "237779\n",
            "1000\n",
            "precision 0.5663694675948622\n",
            "recall 0.48\n",
            "f1-score 0.4159283093975872\n",
            "1000\n",
            "precision 0.5978528310287031\n",
            "recall 0.49\n",
            "f1-score 0.41587330534286093\n",
            "1000\n",
            "precision 0.622715276131359\n",
            "recall 0.5666666666666667\n",
            "f1-score 0.5563000809893157\n",
            "1000\n",
            "precision 0.6048095689888859\n",
            "recall 0.44\n",
            "f1-score 0.3673116039752533\n",
            "1000\n",
            "precision 0.6166377240437757\n",
            "recall 0.5166666666666667\n",
            "f1-score 0.5045901070918929\n",
            "**********\n",
            "mean precision 0.6016769735575173\n",
            "mean recall 0.49866666666666665\n",
            "mean f1-score 0.45200068135938204\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
